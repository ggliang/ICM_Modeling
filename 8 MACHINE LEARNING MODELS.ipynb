{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Feb 16 21:25:20 2020\n",
    "\n",
    "@author: wanghaochen\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb  # 使用XGBoost特征选择及处理多分类\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import plot_importance\n",
    "from sklearn.svm import SVC  # 使用SVM处理多分类\n",
    "from sklearn.neighbors import KNeighborsClassifier  # 使用K-近邻分类处理多分类\n",
    "from sklearn.linear_model import LogisticRegression  # 使用逻辑回归处理多分类\n",
    "from sklearn.linear_model import SGDClassifier  # 使用随机梯度下降处理多分类\n",
    "from sklearn.tree import DecisionTreeClassifier  # 使用决策树处理多分类\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler  # 归一化，标准化\n",
    "\n",
    "path = './Dataset/'\n",
    "\n",
    "Indicators_all = pd.read_excel(path + 'Indicators_all.xlsx', sheet_name='All')\n",
    "Result = Indicators_all[['Result']]\n",
    "Indicators = Indicators_all.iloc[:, 2:]\n",
    "\n",
    "# 使用 XGBoost 挑选对比赛结果有较大影响的特征\n",
    "#利用xgb.train中的get_score得到weight，gain，以及cover\n",
    "params = {'max_depth': 7, 'n_estimators': 80, 'learning_rate': 0.01, 'nthread': 4,\n",
    "          'subsample': 1.0, 'colsample_bytree': 0.5, 'min_child_weight': 3, 'seed': 1301}\n",
    "xgtrain = xgb.DMatrix(Indicators, label=Result)\n",
    "model = xgb.train(params, xgtrain, num_boost_round=100)\n",
    "'''\n",
    "* \"weight\" is the number of times a feature appears in a tree\n",
    "* \"gain\" is the average gain of splits which use the feature\n",
    "* \"cover\" is the average coverage of splits which use the feature\n",
    "'''\n",
    "# 利用plot_importance画出各个特征的重要性排序\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "plot_importance(model, title='Feature Importance', ax=ax,\n",
    "                max_num_features=Indicators.shape[1])\n",
    "plt.show()\n",
    "importance = model.get_fscore()\n",
    "score_var = list(importance)\n",
    "score_val = list(importance.values())\n",
    "score = pd.DataFrame({'Feature': score_var, 'Score': score_val})\n",
    "score = score.sort_values(by='Score', ascending=False)\n",
    "score_var = list(score['Feature'])\n",
    "score_val = list(score['Score'])\n",
    "# 选取了重要性排名前 40% 的变量，舍弃重要性排名后 60% 的变量\n",
    "_n = int(0.4 * len(score_var))\n",
    "selected_var = score_var[: _n]\n",
    "Indicators_selected = Indicators[selected_var]\n",
    "# 使用 PCA 对选取的数据做进一步的降维处理\n",
    "pca = PCA(n_components=2)  # 加载PCA算法，设置降维后主成分数目为2\n",
    "Indicators_reduced = pca.fit_transform(Indicators)  # 对原样本进行降维\n",
    "print(sum(pca.explained_variance_ratio_))\n",
    "\n",
    "Indicators_reduced = pca.fit_transform(Indicators_selected)  # 对筛选后的样本进行降维\n",
    "print(sum(pca.explained_variance_ratio_))\n",
    "\n",
    "# Ureduce=pca.components_ # 各个特征的权重系数\n",
    "Indicators_reduced = pd.DataFrame(Indicators_reduced, columns=['comp_0', 'comp_1'])\n",
    "\n",
    "# 标准化\n",
    "scaler = StandardScaler()\n",
    "Indicators_reduced_scaled = Indicators_reduced.copy(deep=True)\n",
    "Indicators_reduced_scaled['comp_0'] = scaler.fit_transform(Indicators_reduced['comp_0'].values.reshape(-1, 1))\n",
    "Indicators_reduced_scaled['comp_1'] = scaler.fit_transform(Indicators_reduced['comp_1'].values.reshape(-1, 1))\n",
    "Indicator_train_scaled = Indicators_reduced_scaled.iloc[:train_num, :]\n",
    "Indicator_test_scaled = Indicators_reduced_scaled.iloc[train_num:, :]\n",
    "# 划分训练数据集和测试数据集 0.7 : 0.3\n",
    "train_num = int(0.7 * Indicators_reduced.shape[0])\n",
    "Indicator_train = Indicators_reduced.iloc[:train_num, :]\n",
    "Result_train = Result.iloc[:train_num, :]\n",
    "\n",
    "Indicator_test = Indicators_reduced.iloc[train_num:, :]\n",
    "Result_test = Result.iloc[train_num:, :]\n",
    "# 支持向量机预测\n",
    "clf_svm = SVC(kernel='rbf', gamma=0.05, C=1, probability=False, tol=0.0001,\n",
    "              decision_function_shape='ovo', max_iter=-1, random_state=None)\n",
    "clf_svm.fit(Indicator_train, Result_train)\n",
    "pred_clf_svm_train = clf_svm.predict(Indicator_train)\n",
    "pred_clf_svm_test = clf_svm.predict(Indicator_test)\n",
    "\n",
    "accuracy_svm_train = sum(np.asfarray(pred_clf_svm_train) == np.asfarray(Result_train['Result'])) / np.size(Result_train)\n",
    "accuracy_svm_test = sum(np.asfarray(pred_clf_svm_test) == np.asfarray(Result_test['Result'])) / np.size(Result_test)\n",
    "# XGBoost预测\n",
    "params = {\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'multi:softmax',  # 多分类的问题\n",
    "    'num_class': 3,  # 类别数，与 multisoftmax 并用\n",
    "    'gamma': 0.3,  # 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。\n",
    "    'max_depth': 10,  # 构建树的深度，越大越容易过拟合\n",
    "    'lambda': 1,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n",
    "    'subsample': 0.7,  # 随机采样训练样本\n",
    "    'colsample_bytree': 0.7,  # 生成树时进行的列采样\n",
    "    'min_child_weight': 2,\n",
    "    'silent': 1,  # 设置成1则没有运行信息输出，最好是设置为0.\n",
    "    'eta': 0.6,  # 如同学习率\n",
    "    'seed': 1,\n",
    "    'nthread': 1,  # cpu 线程数\n",
    "}\n",
    "dtrain = xgb.DMatrix(Indicator_train, label=Result_train)\n",
    "dtest = xgb.DMatrix(Indicator_test)  #, label=Result_test)\n",
    "num_rounds = 500\n",
    "clf_xgb = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "pred_clf_xgb_train = clf_xgb.predict(dtrain)\n",
    "pred_clf_xgb_test = clf_xgb.predict(dtest)\n",
    "\n",
    "accuracy_xgb_train = sum(np.asfarray(pred_clf_xgb_train) == np.asfarray(Result_train['Result'])) / np.size(Result_train)\n",
    "accuracy_xgb_test = sum(np.asfarray(pred_clf_xgb_test) == np.asfarray(Result_test['Result'])) / np.size(Result_test)\n",
    "# k-近邻分类预测\n",
    "clf_knc = KNeighborsClassifier(n_neighbors=1)\n",
    "clf_knc.fit(Indicator_train, Result_train)\n",
    "\n",
    "pred_clf_knc_train = clf_knc.predict(Indicator_train)\n",
    "pred_clf_knc_test = clf_knc.predict(Indicator_test)\n",
    "\n",
    "accuracy_knc_train = sum(np.asfarray(pred_clf_knc_train) == np.asfarray(Result_train['Result'])) / np.size(Result_train)\n",
    "accuracy_knc_test = sum(np.asfarray(pred_clf_knc_test) == np.asfarray(Result_test['Result'])) / np.size(Result_test)\n",
    "\n",
    "# 逻辑回归分类预测 # 效果很差\n",
    "clf_lor = LogisticRegression(penalty='l2', C=10, multi_class='multinomial', solver='saga')\n",
    "clf_lor.fit(Indicator_train_scaled, Result_train)\n",
    "\n",
    "pred_clf_lor_train = clf_lor.predict(Indicator_train_scaled)\n",
    "pred_clf_lor_test = clf_lor.predict(Indicator_test_scaled)\n",
    "\n",
    "accuracy_lor_train = sum(np.asfarray(pred_clf_lor_train) == np.asfarray(Result_train['Result'])) / np.size(Result_train)\n",
    "accuracy_lor_test = sum(np.asfarray(pred_clf_lor_test) == np.asfarray(Result_test['Result'])) / np.size(Result_test)\n",
    "\n",
    "# 随机梯度下降分类预测 # 拟合很差 但是预测较好\n",
    "clf_sgdv = SGDClassifier(max_iter=1000, random_state=100)\n",
    "clf_sgdv.fit(Indicator_train_scaled, Result_train)\n",
    "\n",
    "pred_clf_sgdv_train = clf_sgdv.predict(Indicator_train)\n",
    "pred_clf_sgdv_test = clf_sgdv.predict(Indicator_test)\n",
    "\n",
    "accuracy_sgdv_train = sum(np.asfarray(pred_clf_sgdv_train) == np.asfarray(Result_train['Result'])) / np.size(\n",
    "    Result_train)\n",
    "accuracy_sgdv_test = sum(np.asfarray(pred_clf_sgdv_test) == np.asfarray(Result_test['Result'])) / np.size(Result_test)\n",
    "# 决策树分类预测\n",
    "clf_tree = DecisionTreeClassifier()  #criterion='entropy'\n",
    "clf_tree.fit(Indicator_train_scaled, Result_train)\n",
    "\n",
    "pred_clf_tree_train = clf_sgdv.predict(Indicator_train)\n",
    "pred_clf_tree_test = clf_sgdv.predict(Indicator_test)\n",
    "\n",
    "accuracy_tree_train = sum(np.asfarray(pred_clf_tree_train) == np.asfarray(Result_train['Result'])) / np.size(\n",
    "    Result_train)\n",
    "accuracy_tree_test = sum(np.asfarray(pred_clf_tree_test) == np.asfarray(Result_test['Result'])) / np.size(Result_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}